{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import e\n",
        "\n",
        "class Node:\n",
        "\n",
        "\n",
        "    def __init__(self, x, gradient, hessian, idxs, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
        "\n",
        "        self.x, self.gradient, self.hessian = x, gradient, hessian\n",
        "        self.idxs = idxs\n",
        "        self.depth = depth\n",
        "        self.min_leaf = min_leaf\n",
        "        self.lambda_ = lambda_\n",
        "        self.gamma  = gamma\n",
        "        self.min_child_weight = min_child_weight\n",
        "        self.row_count = len(idxs)\n",
        "        self.col_count = x.shape[1]\n",
        "        self.subsample_cols = subsample_cols\n",
        "        self.eps = eps\n",
        "        self.column_subsample = np.random.permutation(self.col_count)[:round(self.subsample_cols*self.col_count)]\n",
        "\n",
        "        self.val = self.compute_gamma(self.gradient[self.idxs], self.hessian[self.idxs])\n",
        "\n",
        "        self.score = float('-inf')\n",
        "        self.find_varsplit()\n",
        "\n",
        "\n",
        "    def compute_gamma(self, gradient, hessian):\n",
        "\n",
        "        return(-np.sum(gradient)/(np.sum(hessian) + self.lambda_))\n",
        "\n",
        "    def find_varsplit(self):\n",
        "\n",
        "        for c in self.column_subsample: self.find_greedy_split(c)\n",
        "        if self.is_leaf: return\n",
        "        x = self.split_col\n",
        "        lhs = np.nonzero(x <= self.split)[0]\n",
        "        rhs = np.nonzero(x > self.split)[0]\n",
        "        self.lhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[lhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
        "        self.rhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[rhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
        "\n",
        "    def find_greedy_split(self, var_idx):\n",
        "        x = self.x[self.idxs, var_idx]\n",
        "\n",
        "        for r in range(self.row_count):\n",
        "            lhs = x <= x[r]\n",
        "            rhs = x > x[r]\n",
        "\n",
        "            lhs_indices = np.nonzero(x <= x[r])[0]\n",
        "            rhs_indices = np.nonzero(x > x[r])[0]\n",
        "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf\n",
        "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
        "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
        "\n",
        "            curr_score = self.gain(lhs, rhs)\n",
        "            if curr_score > self.score:\n",
        "                self.var_idx = var_idx\n",
        "                self.score = curr_score\n",
        "                self.split = x[r]\n",
        "\n",
        "    def weighted_qauntile_sketch(self, var_idx):\n",
        "\n",
        "        x = self.x[self.idxs, var_idx]\n",
        "        hessian_ = self.hessian[self.idxs]\n",
        "        df = pd.DataFrame({'feature':x,'hess':hessian_})\n",
        "\n",
        "        df.sort_values(by=['feature'], ascending = True, inplace = True)\n",
        "        hess_sum = df['hess'].sum()\n",
        "        df['rank'] = df.apply(lambda x : (1/hess_sum)*sum(df[df['feature'] < x['feature']]['hess']), axis=1)\n",
        "\n",
        "        for row in range(df.shape[0]-1):\n",
        "            # look at the current rank and the next ran\n",
        "            rk_sk_j, rk_sk_j_1 = df['rank'].iloc[row:row+2]\n",
        "            diff = abs(rk_sk_j - rk_sk_j_1)\n",
        "            if(diff >= self.eps):\n",
        "                continue\n",
        "\n",
        "            split_value = (df['rank'].iloc[row+1] + df['rank'].iloc[row])/2\n",
        "            lhs = x <= split_value\n",
        "            rhs = x > split_value\n",
        "\n",
        "            lhs_indices = np.nonzero(x <= split_value)[0]\n",
        "            rhs_indices = np.nonzero(x > split_value)[0]\n",
        "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf\n",
        "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
        "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
        "\n",
        "            curr_score = self.gain(lhs, rhs)\n",
        "            if curr_score > self.score:\n",
        "                self.var_idx = var_idx\n",
        "                self.score = curr_score\n",
        "                self.split = split_value\n",
        "\n",
        "    def gain(self, lhs, rhs):\n",
        "\n",
        "        gradient = self.gradient[self.idxs]\n",
        "        hessian  = self.hessian[self.idxs]\n",
        "\n",
        "        lhs_gradient = gradient[lhs].sum()\n",
        "        lhs_hessian  = hessian[lhs].sum()\n",
        "\n",
        "        rhs_gradient = gradient[rhs].sum()\n",
        "        rhs_hessian  = hessian[rhs].sum()\n",
        "\n",
        "        gain = 0.5 *( (lhs_gradient**2/(lhs_hessian + self.lambda_)) + (rhs_gradient**2/(rhs_hessian + self.lambda_)) - ((lhs_gradient + rhs_gradient)**2/(lhs_hessian + rhs_hessian + self.lambda_))) - self.gamma\n",
        "        return(gain)\n",
        "\n",
        "    @property\n",
        "    def split_col(self):\n",
        "\n",
        "        return self.x[self.idxs , self.var_idx]\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self):\n",
        "\n",
        "        return self.score == float('-inf') or self.depth <= 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.array([self.predict_row(xi) for xi in x])\n",
        "\n",
        "    def predict_row(self, xi):\n",
        "        if self.is_leaf:\n",
        "            return(self.val)\n",
        "\n",
        "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
        "        return node.predict_row(xi)\n",
        "\n",
        "\n",
        "class XGBoostTree:\n",
        "\n",
        "    def fit(self, x, gradient, hessian, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
        "        self.dtree = Node(x, gradient, hessian, np.array(np.arange(len(x))), subsample_cols, min_leaf, min_child_weight, depth, lambda_, gamma, eps)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.dtree.predict(X)\n",
        "\n",
        "\n",
        "class XGBoostClassifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.estimators = []\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    # first order gradient logLoss\n",
        "    def grad(self, preds, labels):\n",
        "        preds = self.sigmoid(preds)\n",
        "        return(preds - labels)\n",
        "\n",
        "    # second order gradient logLoss\n",
        "    def hess(self, preds, labels):\n",
        "        preds = self.sigmoid(preds)\n",
        "        return(preds * (1 - preds))\n",
        "\n",
        "    @staticmethod\n",
        "    def log_odds(column):\n",
        "        binary_yes = np.count_nonzero(column == 1)\n",
        "        binary_no  = np.count_nonzero(column == 0)\n",
        "        return(np.log(binary_yes/binary_no))\n",
        "\n",
        "\n",
        "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 7, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 20, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
        "        self.X, self.y = X, y\n",
        "        self.depth = depth\n",
        "        self.subsample_cols = subsample_cols\n",
        "        self.eps = eps\n",
        "        self.min_child_weight = min_child_weight\n",
        "        self.min_leaf = min_leaf\n",
        "        self.learning_rate = learning_rate\n",
        "        self.boosting_rounds = boosting_rounds\n",
        "        self.lambda_ = lambda_\n",
        "        self.gamma  = gamma\n",
        "\n",
        "        self.base_pred = np.full((X.shape[0], 1), 1).flatten().astype('float64')\n",
        "\n",
        "        for booster in range(self.boosting_rounds):\n",
        "            Grad = self.grad(self.base_pred, self.y)\n",
        "            Hess = self.hess(self.base_pred, self.y)\n",
        "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
        "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
        "            self.estimators.append(boosting_tree)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "\n",
        "        for estimator in self.estimators:\n",
        "            pred += self.learning_rate * estimator.predict(X)\n",
        "\n",
        "        return(self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred))\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        for estimator in self.estimators:\n",
        "            pred += self.learning_rate * estimator.predict(X)\n",
        "\n",
        "        predicted_probas = self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred)\n",
        "        preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)\n",
        "        return(preds)\n"
      ],
      "metadata": {
        "id": "3USr6_ODysMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_recall_curve, auc\n",
        "\n",
        "# Ensure XGBoostClassifier is defined before usage\n",
        "# Assuming XGBoostClassifier is already implemented as provided\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Train.csv\")  # Replace with your actual dataset\n",
        "\n",
        "# Split target and features\n",
        "y = df.iloc[:, 0]   # First column is the target variable\n",
        "X = df.iloc[:, 1:]  # Remaining columns are features\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Implementing the specific Isolation Forest model\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=310,\n",
        "    contamination=0.08362674314960916,\n",
        "    random_state=42\n",
        ")\n",
        "iso_forest.fit(X_train)\n",
        "\n",
        "# Generate Anomaly Scores as a New Feature (avoiding SettingWithCopyWarning)\n",
        "X_train = X_train.copy()\n",
        "X_test = X_test.copy()\n",
        "X_train[\"anomaly_score\"] = iso_forest.decision_function(X_train)\n",
        "X_test[\"anomaly_score\"] = iso_forest.decision_function(X_test)\n",
        "\n",
        "# Implementing the specific XGBoost model\n",
        "xgb_model = XGBoostClassifier()\n",
        "xgb_model.fit(X_train.values, y_train.values)  # Ensure numpy format if necessary\n",
        "\n",
        "# Predict Probabilities\n",
        "y_pred_prob = xgb_model.predict_proba(X_test.values)  # Ensure correct format\n",
        "y_pred_prob = y_pred_prob.flatten()  # Ensure it's 1D\n",
        "\n",
        "# Adjust Decision Threshold\n",
        "threshold = 0.4  # Can be tuned further\n",
        "y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftYFrZmfu2Jt",
        "outputId": "f7f48471-f120-4190-9dce-783cf71b2891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9771\n",
            "F1 Score: 0.4186\n",
            "Precision-Recall AUC: 0.4888\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1060\n",
            "           1       0.75      0.29      0.42        31\n",
            "\n",
            "    accuracy                           0.98      1091\n",
            "   macro avg       0.86      0.64      0.70      1091\n",
            "weighted avg       0.97      0.98      0.97      1091\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust Decision Threshold\n",
        "threshold = 0.25  # Can be tuned further\n",
        "y_pred = (y_pred_prob >= threshold).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print Results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WywwbUXvVzx",
        "outputId": "65973d2e-4070-4935-82ae-d8164f36d1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9734\n",
            "F1 Score: 0.4727\n",
            "Precision-Recall AUC: 0.4888\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1060\n",
            "           1       0.54      0.42      0.47        31\n",
            "\n",
            "    accuracy                           0.97      1091\n",
            "   macro avg       0.76      0.70      0.73      1091\n",
            "weighted avg       0.97      0.97      0.97      1091\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g09jEu2CfzOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}